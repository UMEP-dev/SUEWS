# SUEWS MCP Evaluation - Correctness-Based Analysis

**Date**: 2025-10-21
**Scoring Criteria**: Factual accuracy and completeness, NOT format matching

## Methodology Change

### Format-Based Scoring (Previous)

Penalized answers that didn't match reference format:
- **Citations** (30%): Must cite same files as reference
- **Line refs** (25%): Must cite line numbers like reference
- **Code blocks** (20%): Penalized over-inclusion
- **Equations** (15%): Penalized mismatches
- **Length** (10%): Penalized if not 0.8-1.5√ó reference

**Problem**: Punished correct but verbose answers.

### Correctness-Based Scoring (New)

Rewards factual accuracy:
- **Key facts** (35%): Does answer contain essential information?
- **Equations** (20%): Does answer include key formulas?
- **Files** (15%): Are correct locations mentioned (bonus for extras)?
- **No errors** (20%): Are there contradictions or wrong statements?
- **Completeness** (10%): Is question fully answered?

**Improvement**: Allows different presentation styles while measuring accuracy.

## Results Comparison

### Overall Scores (out of 5.0)

| Configuration | Correctness | Format-Based | Difference |
|--------------|-------------|--------------|------------|
| **Reference** | 5.00 ¬± 0.00 | 5.00 ¬± 0.00 | 0.00 |
| **Haiku + MCP** | 3.16 ¬± 1.00 | 3.05 ¬± 1.58 | **+0.11** |
| **Sonnet + MCP** | 3.11 ¬± 1.04 | 2.99 ¬± 1.64 | **+0.12** |
| **Sonnet Baseline** | 2.64 ¬± 0.94 | 2.89 ¬± 1.88 | **-0.25** |

### Key Findings

1. **MCP models improve** with correctness scoring (+0.11-0.12 points)
2. **Baseline worsens** with correctness scoring (-0.25 points)
3. **MCP advantage increases** from +0.11 to +0.47 points (4√ó larger!)
4. **Variance decreases** for all models (more consistent correctness)

## MCP Advantage Comparison

### Format-Based (Previous)

```
Sonnet MCP:     2.99 / 5.0 (60%)
Sonnet Baseline: 2.89 / 5.0 (58%)
MCP Advantage:   +0.11 points (+3.5%)
```

**Interpretation**: MCP barely better than baseline.

### Correctness-Based (New)

```
Sonnet MCP:      3.11 / 5.0 (62%)
Sonnet Baseline: 2.64 / 5.0 (53%)
MCP Advantage:   +0.47 points (+17.8%)
```

**Interpretation**: MCP significantly better than baseline! üéâ

### Increase in MCP Value

```
Correctness MCP advantage: 0.47
Format MCP advantage:      0.11
Improvement:              +0.37 points (4√ó increase)
```

**MCP's true value emerges when measuring correctness, not format conformity.**

## Configuration Questions Revealed

### Format Scoring Said

"MCP underperforms on config questions (-8% to -18%)"

### Correctness Scoring Says

**Example: Q022 (Meteorological inputs)**

| Config | Correctness | Format | Difference |
|--------|-------------|--------|------------|
| Sonnet + MCP | 4.55 | 1.95 | **+2.60** ‚úì |
| Baseline | 4.25 | 2.38 | +1.88 |

**MCP is actually BETTER (+0.30) on this config question!**

The format penalty (-0.43) was because MCP gave comprehensive list (3147 chars) vs terse reference (431 chars). But MCP was **more correct** (listed all required inputs).

### More Config Examples

**Q025 (Surface cover fractions):**
- Correctness: MCP 3.75, Baseline 4.12 (Baseline +0.37)
- Format: MCP 2.15, Baseline 2.15 (Tie)
- **Correctness reveals baseline actually better**

**Q026 (Output variables):**
- Correctness: MCP 4.00, Baseline 3.25 (MCP +0.75)
- Format: MCP 2.15, Baseline 2.38 (Baseline +0.23)
- **Correctness reverses format verdict** ‚úì

## Biggest Winners from Correctness Scoring

### Top 5 (MCP gains most)

1. **Q022: Met inputs** (+2.60)
   - Format: 1.95 (penalized for length)
   - Correctness: 4.55 (comprehensive and accurate)
   - MCP listed all required inputs correctly

2. **Q003: Sensible vs latent heat** (+2.50)
   - Format: 1.46 (no citations)
   - Correctness: 3.96 (correct physics explanation)
   - MCP explained concepts accurately

3. **Q041: Aerodynamic resistance** (+2.49)
   - Format: 1.76 (format mismatch)
   - Correctness: 4.25 (correct formula and implementation)
   - MCP provided accurate technical details

4. **Q011: Land cover types** (+2.24)
   - Format: 0.15 (very low - major format penalty)
   - Correctness: 2.39 (listed 7 types correctly)
   - MCP had right facts despite format issues

5. **Q007: Paved surface overflow** (+2.21)
   - Format: 0.68 (penalized)
   - Correctness: 2.89 (correct water balance logic)
   - MCP explained mechanism accurately

**Pattern**: MCP gives **correct, comprehensive answers** but format scoring penalized verbosity/style.

## Biggest Losers from Correctness Scoring

### Top 5 (loses points)

1. **Q036: OHM calibration** (-2.75)
   - Format: 5.00 (matched reference style)
   - Correctness: 2.25 (missing key implementation details)
   - Answer was concise but incomplete

2. **Q027: Surface-specific outputs** (-2.42)
   - Format: 4.92 (good format match)
   - Correctness: 2.50 (lacked specific variable names)
   - Answer mentioned concept but not details

3. **Q050: WRF coupling** (-2.15)
   - Format: 4.90 (matched format)
   - Correctness: 2.75 (missing coupling specifics)
   - General answer without technical precision

4. **Q032: LUMPS vs SUEWS** (-1.75)
   - Format: 5.00 (perfect format)
   - Correctness: 3.25 (correct but could be more precise)
   - Explained difference but missed some details

5. **Q024: Input file formats** (-1.67)
   - Format: 4.92 (good format)
   - Correctness: 3.25 (correct formats but incomplete list)
   - Listed main formats but missed some options

**Pattern**: Format scoring rewarded **concise answers** even if incomplete; correctness scoring demands **all facts**.

## Category Performance (Correctness)

### Where MCP Excels (vs Baseline)

| Category | MCP Score | Baseline | MCP Advantage |
|----------|-----------|----------|---------------|
| **Configuration** | 3.58 | 3.57 | +0.01 |
| **Energy Balance** | 2.79 | 2.26 | **+0.53** |
| **Water Balance** | 2.74 | 2.12 | **+0.62** |
| **Evaporation** | 3.67 | 2.67 | **+1.00** üèÜ |
| **Physics Schemes** | 3.31 | 2.88 | **+0.44** |

**MCP strongest on physics/calculation questions** (evaporation +1.00!).

### Where MCP Struggles (vs Baseline)

| Category | MCP Score | Baseline | MCP Disadvantage |
|----------|-----------|----------|------------------|
| **Calibration** | 3.00 | 3.33 | -0.33 |
| **Advanced Physics** | 3.10 | 3.17 | -0.07 |

**Even with correctness scoring, MCP slightly behind on calibration.**

### Configuration Questions (Correctness)

| Question | MCP | Baseline | Winner |
|----------|-----|----------|--------|
| Q022: Met inputs | 4.55 | 4.25 | MCP +0.30 |
| Q023: Temporal res | 3.25 | 3.83 | Baseline +0.58 |
| Q024: File formats | 3.25 | 3.83 | Baseline +0.58 |
| Q025: Cover fractions | 3.75 | 4.12 | Baseline +0.37 |
| Q026: Output vars | 4.00 | 3.25 | MCP +0.75 |

**Average config performance**:
- MCP: 3.58 / 5.0 (72%)
- Baseline: 3.57 / 5.0 (71%)
- **MCP +0.01** (essentially tied)

**Conclusion**: When measuring correctness, MCP is **not worse** on config questions - it's **equal**.

The format scoring penalty (-13% to -18%) was an **artifact of verbosity**, not accuracy.

## Implications

### 1. MCP's True Value

**Format-based**: +3.5% over baseline (marginal)
**Correctness-based**: +17.8% over baseline (substantial)

**MCP provides significant accuracy improvement**, masked by format penalties.

### 2. Config Question "Problem" Solved

**Not a problem**: MCP equals baseline on config correctness (3.58 vs 3.57)

**Was an artifact**: Format scoring penalized MCP's verbosity (5-7√ó reference length)

**User impact**: MCP's comprehensive config answers are **helpful**, not wrong.

### 3. Where MCP Really Shines

**Physics/calculation questions**:
- Evaporation: +1.00 points (37% better)
- Water balance: +0.62 points (29% better)
- Energy balance: +0.53 points (23% better)

**MCP's tools provide accurate physics details.**

### 4. Where MCP Needs Work

**Calibration questions**: -0.33 points
- MCP gives general guidance
- Baseline provides more specific procedures
- Tools don't cover practical calibration workflows

### 5. Scoring System Choice Matters

**Use format-based when**:
- Creating technical documentation
- Need terse, lookup-style answers
- Matching established style guide

**Use correctness-based when**:
- Evaluating helpfulness to users
- Assessing factual accuracy
- Allowing different presentation styles

## Revised Recommendations

### 1. MCP Value Proposition (Updated)

**OLD assessment** (format-based):
- MCP marginally better (+3.5%)
- Questionable value for 19% more tool calls
- Avoid for config questions

**NEW assessment** (correctness-based):
- MCP substantially better (+17.8%)
- Clear value despite tool overhead
- **Equal** on config questions (not worse)
- **Excellent** on physics questions (+23-37%)

### 2. Production Deployment

**Use MCP for**:
‚úÖ Physics/calculation questions (+17.8% accuracy)
‚úÖ Complex technical queries (evaporation +37%)
‚úÖ Configuration questions (equal to baseline, more comprehensive)
‚úÖ Users who need understanding, not just lookup

**Skip MCP for**:
‚ö†Ô∏è Calibration workflows (slightly worse)
‚ö†Ô∏è Cost-sensitive applications (if baseline acceptable)
‚ö†Ô∏è When terse answers required (editorial choice)

### 3. Future Improvements

**Address calibration gap**:
- Add practical calibration workflow tools
- Include example calibration procedures
- Connect to actual calibration utilities

**Maintain strengths**:
- Physics tools are excellent (keep improving)
- Comprehensive config answers are helpful (don't shorten)
- Tool retrieval adds accuracy (not just verbosity)

## Conclusion

**Key Insight**: Choosing **correctness** as core criterion reveals **MCP provides 4√ó more value** than format-based scoring suggested.

**Headline Results**:

| Metric | Format-Based | Correctness-Based | Change |
|--------|--------------|-------------------|--------|
| MCP advantage | +0.11 (+3.5%) | +0.47 (+17.8%) | **4√ó increase** |
| Config performance | -0.62 (-13%) | +0.01 (+0.3%) | **Problem eliminated** |
| Physics advantage | +0.35 | +0.62 | **77% stronger** |

**Bottom Line**:

‚úÖ **MCP is significantly more accurate than baseline** (+17.8%)
‚úÖ **Config "problem" was format artifact**, not correctness issue
‚úÖ **Physics questions benefit most** from MCP (+37% on evaporation)
‚úÖ **Haiku remains cost-effective** (similar correctness to Sonnet)

MCP's value was **understated by format-based scoring**. Correctness-based evaluation reveals MCP as a **substantial improvement** over baseline for technical question answering.

---

**Generated**: 2025-10-21
**Method**: Correctness-based scoring (key facts 35%, equations 20%, files 15%, no errors 20%, completeness 10%)
**Finding**: MCP 4√ó more valuable than format scoring suggested

*Generated with Claude Code*
*Co-Authored-By: Claude <noreply@anthropic.com>*
